SUPERVISOR: Execute Task 0-1
SUPERVISOR: Execute Task 0/1
module
SUPERVISOR: CALLING train_task: init=1 close=1 exp=cfg/exp/baseline/scannet/25k_pretrain/scannet25k_pretrain.yml env_cfg_path=cfg/env/yujie_ws.yml task_nr=0
2022-04-28 17:15:25 he-legion pytorch_lightning.utilities.seed[482684] INFO Global seed set to 42
Copy cfg/env/yujie_ws.yml to /home/he/projects/cl_seg_res/scannet25k_pretrain/2022-04-28T17:15:25_test/scannet25k_pretrain.yml
==========================================================================================
Summary TaskGenerator Tasks: 1
  00 Name: Train_25k
==========================================================================================
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO TimeLimitCallback is set to 60min
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:287: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO Using 16bit native Automatic Mixed Precision (AMP)
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO GPU available: True, used: True
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO TPU available: False, using: 0 TPU cores
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO IPU available: False, using: 0 IPUs
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO HPU available: False, using: 0 HPUs
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.
  rank_zero_deprecation(
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.seed[482684] INFO Global seed set to 42
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.distributed[482684] INFO Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
2022-04-28 17:15:26 he-legion torch.distributed.distributed_c10d[482684] INFO Added key: store_based_barrier_key:1 to store for rank: 0
2022-04-28 17:15:26 he-legion torch.distributed.distributed_c10d[482684] INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
2022-04-28 17:15:26 he-legion pytorch_lightning.utilities.rank_zero[482684] INFO ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

2022-04-28 17:15:26 he-legion pytorch_lightning.loggers.tensorboard[482684] WARNING Missing logger folder: /home/he/projects/cl_seg_res/scannet25k_pretrain/2022-04-28T17:15:25_test/tensorboard
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /home/he/projects/cl_seg_res/scannet25k_pretrain/2022-04-28T17:15:25_test exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
2022-04-28 17:15:30 he-legion pytorch_lightning.accelerators.gpu[482684] INFO LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
2022-04-28 17:15:30 he-legion pytorch_lightning.callbacks.model_summary[482684] INFO
   | Name                | Type                | Params
-------------------------------------------------------------
0  | model               | FastSCNN            | 1.1 M
1  | _rssb               | ReplayStateSyncBack | 0
2  | train_acc           | Accuracy            | 0
3  | train_aux_acc       | Accuracy            | 0
4  | train_aux_vs_gt_acc | Accuracy            | 0
5  | train_acc_real      | Accuracy            | 0
6  | train_acc_replayed  | Accuracy            | 0
7  | val_acc             | ModuleList          | 0
8  | val_aux_acc         | ModuleList          | 0
9  | val_aux_vs_gt_acc   | ModuleList          | 0
10 | test_acc            | Accuracy            | 0
11 | _teacher            | Teacher             | 0
-------------------------------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params
2.282     Total estimated model params size (MB)
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
============== SET REPLAY DATASET GLOBALS ===============
Training: 0it [00:00, ?it/s]
================ ON_TRAIN_START ==================
 TASK NAME:                             Train_25k
 TASK COUNT:                            0
 CURRENT EPOCH:                         0
 CURRENT EPOCH:                         0
 RSSB STATE:                            tensor([0], device='cuda:0')
 TRAINING DATASET LENGTH:               19921
 VALIDATION DATASET 0 LENGTH:           4981
 =============  ON_TRAIN_START_DONE ===============
Epoch 0:   0%|                                                | 0/24902 [00:00<?, ?it/s]Traceback (most recent call last):
  File "supervisor_new.py", line 80, in <module>
    train_task(init, close, args.exp, env_cfg_path, i, skip=skip, logger_pass=None)
  File "/home/he/projects/cl_seg/train_task_new.py", line 387, in train_task
    _ = trainer.fit(
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1354, in _run_train
    self.fit_loop.run()
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1596, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 278, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py", line 85, in optimizer_step
    closure_result = closure()
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1766, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 344, in training_step
    return self.model(*args, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 82, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/home/he/projects/cl_seg/src/lightning/lightning_new.py", line 246, in training_step
    outputs = self(batch=ba["images"])
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/projects/cl_seg/src/lightning/lightning_new.py", line 143, in forward
    outputs = self.model(batch)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/projects/cl_seg/src/models_asl/fast_scnn.py", line 74, in forward
    x = self._md["extractor"](
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/projects/cl_seg/src/models_asl/fast_scnn.py", line 306, in forward
    x = self.ppm(x)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/projects/cl_seg/src/models_asl/fast_scnn.py", line 244, in forward
    feat1 = self.upsample(self.conv1(self.pool(x, 1)), size)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/projects/cl_seg/src/models_asl/fast_scnn.py", line 163, in forward
    return self.conv(x)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/functional.py", line 2280, in batch_norm
    _verify_batch_size(input.size())
  File "/home/he/miniconda/envs/cl/lib/python3.8/site-packages/torch/nn/functional.py", line 2248, in _verify_batch_size
    raise ValueError("Expected more than 1 value per channel when training, got input size {}".format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 32, 1, 1])
Epoch 0:   0%|          | 0/24902 [00:02<?, ?it/s]